{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Conclusions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXIbulYGoDoR"
      },
      "source": [
        "# Baseline performance.\n",
        "\n",
        "![alt text](https://i.ibb.co/W2GVnCb/Screen-Shot-2021-03-31-at-21-27-42.png)\n",
        "\n",
        "BLEU score: 14.79307847852665"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF6PCGsGoDob"
      },
      "source": [
        "**<font color=blue>Observations:</font>**\n",
        "\n",
        "- First we observe that the different techniques for improving the translation quality are not mutually exclusive.\n",
        "- Below just for the sake of the problem at hand we apply them separately in order to see how much of an improvement each can contribute.\n",
        "- The first to model didn't do that well. Their BLUE score $< 22$ **but** the last one does relatively well (above the lower bound required for sending the homework). That's why allow myself to submit the homework as it is. If it was require to obtain a BLUE score $\\geq 20$ in each model, I appreciate you inform me in order to make the respective fixes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrehbjftoDob"
      },
      "source": [
        "# Model 1: Optimization Enhancement: Learning Rate Decay.\n",
        "\n",
        "![alt text](https://i.ibb.co/J5kymX7/Screen-Shot-2021-03-31-at-21-29-27.png)\n",
        "\n",
        "BLEU score: 13.639520230929117\n",
        "\n",
        "**<font color=blue>Conclusions:</font>**\n",
        "\n",
        "- For learning rate decay we apply out-of-the-box `lr_scheduler.ReduceLROnPlateau` with `patience=2` for dynamic learning rate reduction.\n",
        "- Both loss and scores is worse than the baseline performance. This may due to the fact that:\n",
        "    - Each batch is small (128 sentence, each of length approx. equal to 50), which may lead to more erratic behaviour when updating weights (the gradient descent step).\n",
        "    - The number of epochs is also not that big (10), which doesn't give time the scheduler to reduce the learning rate (it only does so if after `patience` steps it doesn't see improvements in the loss function). This in turn may only lead to additional computation rather that performance enhacements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8g1E2SBoDoc"
      },
      "source": [
        "## Model 2: Word Segmentation for Russian Language\n",
        "\n",
        "Tokenization of russian words is not that simple compared to english words, mainly because of the presence of compound words or word connected by hyphens. In that case word like `какой-то` will give multiple tokens, which separately convey the meaning of neither the original russian nor the target english word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mabgKG-FoDod"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fIrhjooIo5"
      },
      "source": [
        "tokenizer_W = WordPunctTokenizer()\n",
        "def tokenize(x, tokenizer=tokenizer_W):\n",
        "    return tokenizer.tokenize(x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vdZn5oucoDoe"
      },
      "source": [
        "text = \"Не ветер, а какой-то ураган!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ1HkK_joOLa"
      },
      "source": [
        "To deal with this we'll use spacy's out-of-the-box tokenization and text segmentation API that can handle more accurate rules for the russian language.\n",
        "\n",
        "The vocabulary of patterns is obtained from National Russian Language Corpus (НКРЯ). For more details (and props) on the API see [here](https://github.com/aatimofeev/spacy_russian_tokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-j1KRaa9oL4q",
        "outputId": "20fc5712-3b37-4458-a6d7-7af9fd3d3168"
      },
      "source": [
        "!pip install pymorphy2==0.8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.8MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8) (0.6.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Installing collected packages: pymorphy2-dicts, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5WCDRLvoPuP"
      },
      "source": [
        "from spacy.lang.ru import Russian"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e7_bIrUZoRbY",
        "outputId": "9362915a-86f8-4ce1-803d-f4ca084dd74e"
      },
      "source": [
        "!pip install git+https://github.com/aatimofeev/spacy_russian_tokenizer.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/aatimofeev/spacy_russian_tokenizer.git\n",
            "  Cloning https://github.com/aatimofeev/spacy_russian_tokenizer.git to /tmp/pip-req-build-hoav0yqa\n",
            "  Running command git clone -q https://github.com/aatimofeev/spacy_russian_tokenizer.git /tmp/pip-req-build-hoav0yqa\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from spacy-russian-tokenizer==0.1.1) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (54.2.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (3.4.1)\n",
            "Building wheels for collected packages: spacy-russian-tokenizer\n",
            "  Building wheel for spacy-russian-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-russian-tokenizer: filename=spacy_russian_tokenizer-0.1.1-cp37-none-any.whl size=12675 sha256=866ca11f368845def1b6105c7b869ea7e4df3c08fc534d31e045c2ae885cfd05\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5hc1eki8/wheels/37/3b/bb/cfe712f7c0b78cd08f4a2ef122d17748baf9d4bebecf2e5a54\n",
            "Successfully built spacy-russian-tokenizer\n",
            "Installing collected packages: spacy-russian-tokenizer\n",
            "Successfully installed spacy-russian-tokenizer-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR453jrnoVCt"
      },
      "source": [
        "from spacy.lang.ru import Russian\n",
        "from spacy_russian_tokenizer import RussianTokenizer, MERGE_PATTERNS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_yg3yrboaQ3"
      },
      "source": [
        "nlp = Russian()\n",
        "russian_tokenizer = RussianTokenizer(nlp, MERGE_PATTERNS)\n",
        "nlp.add_pipe(russian_tokenizer, name='russian_tokenizer')\n",
        "def rus_tokenize(x, tokenizer=nlp):\n",
        "  tokens = nlp(x.lower())\n",
        "  return [token.text for token in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2th-kp8obgp",
        "outputId": "d0b01f40-7d52-4552-8ff6-6a565a6c2658"
      },
      "source": [
        "tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['не', 'ветер', ',', 'а', 'какой', '-', 'то', 'ураган', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ0zCCxRoc7p",
        "outputId": "c1ce3909-5115-4785-bf4b-61a3bfe525dd"
      },
      "source": [
        "rus_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['не', 'ветер', ',', 'а', 'какой-то', 'ураган', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfoRtTOipB-n"
      },
      "source": [
        "![alt text](https://i.ibb.co/x14Xzgz/Screen-Shot-2021-03-31-at-21-55-01.png)\n",
        "\n",
        "BLEU Score: 14.94341105622846\n",
        "\n",
        "\n",
        "**<font color=blue>Conclusions:</font>**\n",
        "\n",
        "- This model does slightly better than the baseline, which implies that a more correct tokenization of russian words may have helped to put a better correspondence between source and target words.\n",
        "- Repeated experiments may help to elucidate whether this improvements are always expected or not (I mean, the BLUE score here and in the baseline are quite tight).\n",
        "- One reason why we didn't get the expected improvements may be related to the nature of the text we trained on. The texts dealt with are hotel descriptions, which means that the language used is more formal and words such as `какой-то`, `кто-нибудь` are not met that often (are words that transmit some sense of uncertainty; something we wouldn't expect from a hotel description found in booking.com or somewhere else)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRoiJ_WOpP6J"
      },
      "source": [
        "## Model 4: Transformer with Attention.\n",
        "\n",
        "![alt text](https://i.ibb.co/5xdj004/Screen-Shot-2021-03-31-at-22-10-02.png)\n",
        "\n",
        "**<font color=green>BLEU Score: 27.33281166978366</font>**\n",
        "\n",
        "**<font color=blue>Observations:</font>**\n",
        "\n",
        "- Here we implemented Luong's Attention from [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf).\n",
        "- The number of layers chosen is 1 for computational reasons.\n",
        "- The score used for computing the attention values $\\alpha_{ts}$ were computed using the _dot_ alternative: $\\mathrm{score}(h_t, \\overline{h}_s) = h_t^{\\top}\\overline{h}_s$ where $h_t$ is the current decoder hidden state and $\\overline{h}_s$ are all the encoder hidden states. Hence $\\alpha_{ts}$ is computed by softmaxing $\\mathrm{score}(h_t, \\overline{h}_s)$.\n",
        "- Implementation details can be found in the file `my_network_attention.py` (class `Decoder`).\n",
        "\n",
        "**<font color=blue>Conclusions:</font>**\n",
        "\n",
        "- The score is good enough (: \n",
        "- Here we can see why having only a meaningful tokenization is not enough for improving the translation quality: It doesn't prevent the bottleneck caused by only passing just last encoder hidden state to the decoder, as we do in the vanilla transformer. It's advisable to introduce (not limited to this, though) some technique that captures the \"influence\" of each word in the source on the next word we're trying to predict (word alignment). Attention does just that.\n",
        "- Since each source sentence has a length of $\\approx 50$ words/tokens the total number of hidden states produces by the encoder is also $\\approx 50$. Their number is small enough as to calculate attention use them all, and it's also large enough to see that how attention allows to capture information from not-that-small sentences. \n",
        "- Further experiments relatd to attenton may include:\n",
        "  - Trying different techniques for calculating $\\mathrm{score}(h_t, \\overline{h}_s)$.\n",
        "  - Trying [Bahdanau's Attention](https://arxiv.org/pdf/1409.0473.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drcJukcpoeHu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}